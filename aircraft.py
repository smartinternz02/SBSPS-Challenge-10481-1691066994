# -*- coding: utf-8 -*-
"""AirCraft.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T3qT8T9qzO6_6XndNYtgeSHnIVNJeuLU
"""

!pip install keras_preprocessing

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.image import imread
import cv2
import random
import os
from os import listdir
from PIL import Image
from sklearn.preprocessing import label_binarize, LabelBinarizer
from keras.preprocessing import image
from tensorflow.keras.optimizers import Adam
from keras_preprocessing.image import img_to_array
from keras.utils import to_categorical
# Your code using to_categorical here
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Flatten, Dropout, Dense
from sklearn.model_selection import train_test_split
from keras.models import model_from_json

!ls "/content/drive/MyDrive/AIRCRAFT_DATASET"

plt.figure(figsize=(12,12))
path = "/content/drive/MyDrive/AIRCRAFT_DATASET/corrosion"
for i in range(1,17):
           plt.subplot(4,4,i)
           plt.tight_layout()
           rand_img = imread(path +'/'+ random.choice(sorted(os.listdir(path))))
           plt.imshow(rand_img)
           plt.xlabel(rand_img.shape[1], fontsize = 10)#width of image
           plt.ylabel(rand_img.shape[0], fontsize = 10)#height of image

#Converting Images to array
def convert_image_to_array(image_dir):
  try:
    image = cv2.imread(image_dir)
    if image is not None :
      image = cv2.resize(image, (256,256))
      #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
      return img_to_array(image)
    else:
      return np.array([])
  except Exception as e:
    print(f"Error : {e}")
    return None

dir = "/content/drive/MyDrive/AIRCRAFT_DATASET"
root_dir = listdir(dir)
image_list, label_list = [], []
all_labels = ['corrosion', 'dents', 'paint_irregularitie' , 'scratches']
binary_labels= [0,1,2]
temp = 1

for directory in root_dir:
  aircraft_image_list = listdir(f"{dir}/{directory}")
  temp = 1
  for files in aircraft_image_list:
    image_path = f"{dir}/{directory}/{files}"
    image_list.append(convert_image_to_array(image_path))
    label_list.append(binary_labels[temp])

# Visualize the number of classes count
label_counts = pd.DataFrame(label_list).value_counts()
label_counts.head()

#Next we will observe the shape of the image.
image_list[0].shape

image_list=np.array(image_list)
image_list.shape

label_list = np.array(label_list)
label_list.shape

import numpy as np
from sklearn.model_selection import train_test_split

# Creating dummy data (replace this with your actual data)
num_samples = 100
# Perform train-test split
x_train, x_test, y_train, y_test = train_test_split(image_list, label_list, test_size=0.2, random_state=10)

# Check the shapes of the resulting arrays
print("x_train shape:", x_train.shape)
print("x_test shape:", x_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

pip install scikit-learn

from sklearn.model_selection import train_test_split
import numpy as np
x_train, x_test, y_train, y_test = train_test_split(np.array(image_list), np.array(label_list), test_size=0.2, random_state=10)
x_train = np.array(x_train, dtype=np.float16) / 255.0  # Normalize pixel values
x_test = np.array(x_test, dtype=np.float16) / 255.0

x_train = x_train.reshape(-1, 256, 256, 3)

import tensorflow as tf

# Assuming you have integer labels in y_train
num_classes = 3  # Change this to the actual number of classes

# Convert integer labels to one-hot encoded vectors
y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)

y_test = to_categorical(y_test)
model = Sequential()
model.add(Conv2D(32, (3, 3), padding="same",input_shape=(256,256,3), activation="relu"))
model.add(MaxPooling2D(pool_size=(3, 3)))
model.add(Conv2D(16, (3, 3), padding="same", activation="relu"))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(8, activation="relu"))
model.add(Dense(3, activation="softmax"))
model.summary()

model.compile(loss = 'categorical_crossentropy', optimizer = Adam(0.0001),metrics=['accuracy'])

#Next we will split the dataset into validation and training data.
# Splitting the training data set into training and validation data sets
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2)

epochs = 50
batch_size = 128
history = model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs,
validation_data = (x_val, y_val))

#Plot the training history
plt.figure(figsize=(12, 5))
plt.plot(history.history['accuracy'], color='r')
plt.plot(history.history['val_accuracy'], color='b')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epochs')
plt.legend(['train', 'val'])
plt.show()

y_pred = model.predict(x_test)